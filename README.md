## Welcome to GitHub

##Toxicity classification

##kaggle competition

Toxic comments contains foul language, derogatory remarks.This could lead to: 

○ Spread of hatred

○ Spread of racial slur

○ Overall tension in communities

○ Attack on individual 

It is abuse of freedom of speech.

Thus has to be monitored and censored on leading social networking sites. 

Two methods used to represent text: 
1. Bag of words using TF-IDF 
2. Word Embedding

Method 1:Using logistic regression
Method 2:Using simple-lstm

view of the project:
Part-1: Overview of Multi-label classification.
Part-2: Problem definition & evaluation metrics.
Part-3: Exploratory data analysis (EDA).
Part-4: Data pre-processing.
Part-5: Multi-label classification techniques.
### Support or Contact

Having trouble with Pages? Check out our [documentation](https://help.github.com/categories/github-pages-basics/) or [contact support](https://github.com/contact) and we’ll help you sort it out.
